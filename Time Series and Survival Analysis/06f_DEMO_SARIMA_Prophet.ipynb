{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 6, Part f: SARIMA and Prophet DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3ed72d58-7719-40d6-a229-73778f445d4a"
    }
   },
   "source": [
    "\n",
    "## Learning Outcomes\n",
    "You should walk away from this notebook with:\n",
    "1. A practical understanding of Autoregressive Integrated Moving Average (ARIMA) models.\n",
    "2. Insight into checking fit of model.\n",
    "3. Learn to create forecasts with ARIMA models in Python.\n",
    "4. A practical understanding of fbprophet\n",
    "5. How to check fit of fbprophet model\n",
    "6. Means of adjusting and improving fbprophet model parameters\n",
    "\n",
    "\n",
    "# Overview: Time Series Modeling Approaches\n",
    "\n",
    "In previous lessons, we explored Python implementations of fundamental time series concepts including stationarity, smoothing, trend, seasonality, and autocorrelation, and built two kinds of models: \n",
    "\n",
    "  * **MA models**: Specify that the current value of the series depends linearly on the series' mean and a set of prior (observed) white noise error terms.\n",
    "  * **AR models**: Specify that the current value of the  series depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term).\n",
    "\n",
    "In the current lesson we will review these concepts as well as combine these two model types into three more complicated time series models: ARMA, ARIMA, and SARIMA. We will then explore a different type of Time Series modeling using __[Facebook Prophet](https://facebook.github.io/prophet/https://facebook.github.io/prophet/)__. \n",
    "\n",
    "__Installation notes:__\n",
    "\n",
    "[Prophet holiday issues](https://stackoverflow.com/questions/60145006/cannot-import-name-easter-from-holidays)\n",
    "\n",
    "To install __[pdarima](https://pypi.org/project/pmdarima/)__ on Anaconda, use:\n",
    "\n",
    "[conda install -c saravji pdarima](https://anaconda.org/saravji/pmdarima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import sys, os\n",
    "sys.path.append(r\"IBM_Specialized_Models/Time Series and Survival Analysis\")\n",
    "os.chdir(\n",
    "    r\"C:/Users/cjr21/OneDrive/Documents/IBM_Specialized_Models/Time Series and Survival Analysis\"\n",
    ")\n",
    "from colorsetup import colors, palette\n",
    "\n",
    "sns.set_palette(palette)\n",
    "os.chdir(\n",
    "    r\"C:/Users/cjr21/OneDrive/Documents/IBM_Specialized_Models/Time Series and Survival Analysis/data\"\n",
    ")\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import prophet\n",
    "import pmdarima as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Time Series Review\n",
    "\n",
    "We've covered lots of material in the previous four lessons. Now is a good time to step back and rehash what we've covered. This will help to both solidify concepts and ensure you're ready to tackle ARMA, ARIMA, and SARIMA models.\n",
    "\n",
    "### Section 1.1: Examples of time series data and modeling:\n",
    "- Hedge fund prediction of stock and index movements\n",
    "- Long and short-term weather forecasting\n",
    "- Business budgeting and trend analysis\n",
    "- Health vitals monitoring\n",
    "- Traffic flows and logistic optimization modeling\n",
    "- Can you think of others?\n",
    "\n",
    "### Section 1.2: Decomposition\n",
    "Time series data can often be decomposed into trend, seasonal, and random fluctuation components.\n",
    "\n",
    "Decomposition\n",
    "\n",
    "- Trends\n",
    "    - Up\n",
    "    - Down\n",
    "    - Flat\n",
    "    - Larger trends can be made up of smaller trends\n",
    "    - There is no defined timeframe for what constitutes a trend as it depends on your goals\n",
    "- Seasonal Effects\n",
    "    - Weekend retail sales spikes\n",
    "    - Holiday shopping\n",
    "    - Energy requirement changes with annual weather patterns\n",
    "    - Note: Twitter spikes when news happens are not seasonal because they aren't regular and predictable\n",
    "- Random Fluctuations\n",
    "    - The human element\n",
    "    - Aggregations of small influencers\n",
    "    - Observation errors\n",
    "    - The smaller this is in relation to Trend and Seasonal, the better we can predict the future\n",
    "\n",
    "### Section 1.3: Additive vs Multiplicative\n",
    "Time series models fall into [two camps](http://www.abs.gov.au/websitedbs/D3310114.nsf/home/Time+Series+Analysis:+The+Basics#HOW%20DO%20I%20KNOW%20WHICH%20DECOMPOSITION):\n",
    "- Additive\n",
    "    - Data = Trend + Seasonal + Random\n",
    "    - What we will be using for our modeling\n",
    "- Multiplicative\n",
    "    - Data = Trend x Seasonal x Random\n",
    "    - As easy to fit as Additive if we take the log\n",
    "        - log(Data) = log(Trend x Seasonal x Random)\n",
    "\n",
    "We should use multiplicative models when the percentage change of our data is more important than the absolute value change (e.g. stocks, commodities); as the trend rises and our values grow, we see amplitude growth in seasonal and random fluctuations. If our seasonality and fluctuations are stable, we likely have an additive model.\n",
    "\n",
    "### Section 1.4: Time Series Modeling Process\n",
    "Time series model selection is driven by the Trend and Seasonal components of our raw data. The general approach for analysis looks like this:\n",
    "\n",
    "1. Plot the data and determine Trends and Seasonality\n",
    "    1. Difference or take the log of the data (multiple times if needed) to remove trends for [certain model applications](https://en.wikipedia.org/wiki/Stationary_process)\n",
    "    1. Stationairity is needed for ARMA models\n",
    "1. Determine if we have additive or multiplicative data patterns\n",
    "1. Select the appropriate algorithm based on the chart below\n",
    "1. Determine if model selection is correct with these tools\n",
    "    - Ljung-Box Test\n",
    "    - Residual Errors (Normal Distribution with zero mean and constant variance-homoskedastic, i.i.d)\n",
    "    - Autocorrelation Function (ACF)\n",
    "    - Partial Autocorrelation Function (PACF)\n",
    "\n",
    "Algorithm | Trend | Seasonal | Correlations\n",
    "---|---|---|---\n",
    "ARIMA | X |X|X\n",
    "SMA Smoothing |X||\n",
    "Simple Exponential Smoothing |X||\n",
    "Seasonal Adjustment |X|X|\n",
    "Holt's Exponential Smoothing |X||\n",
    "Holt-Winters |X|X|\n",
    "\n",
    "### Section 1.5: How to Achieve and Test for Stationarity\n",
    "- The mean of the series is not a function of time.\n",
    "\n",
    "- The variance of the series is not a function of time (homoscedasticity).\n",
    "\n",
    "- The covariance at different lags is not a function of time.\n",
    "\n",
    "[From A Complete Tutorial on Time Series Modeling in R](https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/)\n",
    "\n",
    "- [Info on stationarity](http://www.investopedia.com/articles/trading/07/stationary.asp)\n",
    "- Plotting Rolling Statistics\n",
    "    - Plot the moving average/variance and see if it changes with time. This visual technique can be done on different windows, but isn't as rigorously defensible as the test below.\n",
    "- Augmented Dickey-Fuller Test\n",
    "    - Statistical tests for checking stationarity; the null hypothesis is that the TS is non-stationary. If our test statistic is below an `alpha` value, we _can_ reject the null hypothesis and say that the series is stationary.\n",
    "    \n",
    "    $$ Y_t = \\rho * Y_{t-1} + \\epsilon_t $$\n",
    "    \n",
    "    $$ Y_t - Y_{t-1} = (\\rho - 1) Y_{t - 1} + \\epsilon_t $$\n",
    "    \n",
    "### Section 1.6: Differencing Example\n",
    "\n",
    "This will give us a reminder to how differencing is used to get a stationary series which will be essential to the final piece of the ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d2e075c0-fccd-4d2c-ba33-1ca65629bd24"
    }
   },
   "outputs": [],
   "source": [
    "# create a play dataframe from 1-10 (linear and squared) to test how differencing works\n",
    "play = pd.DataFrame([[x for x in range(1,11)], [x**2 for x in range(1,11)]]).T\n",
    "play.columns = ['original', 'squared']\n",
    "play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9f057e12-cf7f-456e-b1cb-9947c891cc4a"
    }
   },
   "outputs": [],
   "source": [
    "# stationarize linear series (mean and variance don't change for sub-windows)\n",
    "play.original.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,3,figsize = (15,5))\n",
    "axes[0].plot(play.squared)\n",
    "axes[0].set_title('squared')\n",
    "axes[1].plot(play.squared.diff())\n",
    "axes[1].set_title('first diff')\n",
    "axes[2].plot(play.squared.diff().diff())\n",
    "axes[2].set_title('second diff')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This is similar to taking a first-order derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "44e98900-d54d-4e5d-b6b6-5c1f24159247"
    }
   },
   "outputs": [],
   "source": [
    "# stationarize squared series\n",
    "play.squared.diff().diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Notice we need to difference twice on an exponential trend, and every time we do, we lose a bit of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2f7a325d-0c17-4218-aa6d-68e328712d46"
    }
   },
   "outputs": [],
   "source": [
    "# stationarize squared with log\n",
    "np.log(play.squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Works somewhat but certainly not as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,2,figsize = (10,5))\n",
    "axes[0].plot(play.squared)\n",
    "axes[0].set_title('original')\n",
    "axes[1].plot(np.log(play.squared))\n",
    "axes[1].set_title('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d13da4fe-988f-4931-ac85-94b88fdf5a5c"
    }
   },
   "source": [
    "## Data Prep and EDA\n",
    "\n",
    "We'll be looking at [monthly average temperatures between 1907-1972](https://datamarket.com/data/set/22o4/mean-monthly-temperature-1907-1972#!ds=22o4&display=line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbpresent": {
     "id": "ab62c237-c818-4345-82df-40941025361e"
    }
   },
   "outputs": [],
   "source": [
    "# load data and convert to datetime\n",
    "monthly_temp = pd.read_csv('./mean-monthly-temperature-1907-19.csv', \n",
    "                           skipfooter=2, \n",
    "                           infer_datetime_format=True, \n",
    "                           header=0, \n",
    "                           index_col=0, engine='python',\n",
    "                           names=['month', 'temp'])\n",
    "\n",
    "monthly_temp.index = pd.to_datetime(monthly_temp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "19e5de40-e7b6-42e4-b5b3-90f806d7aed7"
    }
   },
   "outputs": [],
   "source": [
    "monthly_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0a23aa65-41fa-476c-af07-d844bc712160"
    }
   },
   "outputs": [],
   "source": [
    "# describe\n",
    "monthly_temp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to annual and plot each\n",
    "plt.rcParams['figure.figsize'] = [14, 4]\n",
    "annual_temp = monthly_temp.resample('A').mean()\n",
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(monthly_temp)\n",
    "axes[1].plot(annual_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both on same figure\n",
    "plt.plot(monthly_temp)\n",
    "plt.plot(annual_temp)\n",
    "plt.grid(b=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e1676068-bb3a-4801-85b0-8ba35c55ea80"
    }
   },
   "outputs": [],
   "source": [
    "# violinplot of months to determine variance and range\n",
    "sns.violinplot(x=monthly_temp.index.month, y=monthly_temp.temp)\n",
    "plt.grid(b=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into 10 chunks\n",
    "chunks = np.split(monthly_temp.temp, indices_or_sections=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(chunks,axis=1)\n",
    "var_vals = np.var(chunks,axis=1)\n",
    "vals = {'mean_vals': mean_vals , 'var_vals': var_vals}\n",
    "mean_var = pd.DataFrame(vals)\n",
    "mean_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbpresent": {
     "id": "318b7e80-f2a6-4d35-8278-8b8d304011ef"
    }
   },
   "outputs": [],
   "source": [
    "# define Dickey-Fuller Test (DFT) function\n",
    "# Null is that unit root is present, rejection means likely stationary\n",
    "import statsmodels.tsa.stattools as ts\n",
    "def dftest(timeseries):\n",
    "    dftest = ts.adfuller(timeseries,)\n",
    "    dfoutput = pd.Series(dftest[0:4], \n",
    "                         index=['Test Statistic','p-value','Lags Used','Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window=12).mean()\n",
    "    rolstd = timeseries.rolling(window=12).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean and Standard Deviation')\n",
    "    plt.grid()\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d043ef61-dc41-46b3-86af-aaca9d21bde0"
    }
   },
   "outputs": [],
   "source": [
    "# run DFT on monthly\n",
    "dftest(monthly_temp.temp)\n",
    "# p-value allows us to reject a unit root: data is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0d4cbef8-0e6c-4074-868d-e28e9c51aaa9"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# run DFT on annual\n",
    "dftest(annual_temp.temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value allows us to *reject* a unit root (i.e. the data is stationary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's an example of non-stationary with DFT results\n",
    "dftest(np.exp(annual_temp.temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important to note that values have strong seasonality and adf test as well as rolling mean may not capture this.\n",
    "# That is why it is always important to pay attention to run sequence plot\n",
    "monthly_temp['lag_12'] = monthly_temp.shift(12)\n",
    "monthly_temp['seasonal_diff'] = monthly_temp.temp - monthly_temp['lag_12']\n",
    "\n",
    "fig,axes = plt.subplots(2,1)\n",
    "axes[0].plot(monthly_temp.temp,label ='original')\n",
    "axes[1].plot(monthly_temp.seasonal_diff,label = 'seasonal diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "81feb119-d24b-45fc-bb19-12a003a78136"
    }
   },
   "source": [
    "## Section 2: SARIMA with Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went through getting stationary data and differencing as that is the last piece of the puzzle that we are missing for understanding ARIMA models. The I stands for \"Integrated\" which just refers to the amount of differcing done on the data.\n",
    "\n",
    "When we are determining our ARIMA model we will come across the following standard inputs:\n",
    "- order(p,d,q):\n",
    "    - p is number of AR terms\n",
    "    - d is number of times that we would difference our data\n",
    "    - q is number of MA terms\n",
    "    \n",
    "When we work with SARIMA models 'S' refers to 'seasonal' and we have the additional standard inputs:\n",
    "- seasonal order(p,d,q):\n",
    "    - p is number of AR terms in regards to seasonal lag\n",
    "    - d is number of times that we would difference our seasonal lag (as seen above)\n",
    "    - q is number of MA terms in regards to seasonal lag\n",
    "    - s is number of periods in a season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder of some good resources:\n",
    "- [ARIMA in R](https://www.otexts.org/fpp/8/5)\n",
    "- [Duke ARIMA Guide](https://people.duke.edu/~rnau/411arim2.htm)\n",
    "- [Great explanation on MA in practice](http://stats.stackexchange.com/questions/164824/moving-average-ma-process-numerical-intuition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rules to highlight from the Duke ARIMA Guide:\n",
    "1. If the series has positive autocorrelations out to a high number of lags, then it probably needs a higher order of differencing\n",
    "2. If the lag-1 autocorrelation is zero or negative, or the autocorrelations are all small and patternless, then the series does not need a higher order of  differencing. If the lag-1 autocorrelation is -0.5 or more negative, the series may be overdifferenced.  BEWARE OF OVERDIFFERENCING!!\n",
    "3. A model with no orders of differencing assumes that the original series is stationary (mean-reverting). A model with one order of differencing assumes that the original series has a constant average trend (e.g. a random walk or SES-type model, with or without growth). A model with two orders of total differencing assumes that the original series has a time-varying trend (e.g. a random trend or LES-type model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "30edf719-cba2-4c09-9f85-dc5ab1042012"
    }
   },
   "source": [
    "### Create Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbpresent": {
     "id": "985b11bd-7d59-4dcf-8729-da9d06ab0078"
    }
   },
   "outputs": [],
   "source": [
    "# define helper plot function for visualization\n",
    "def plots(data, lags=None):\n",
    "    layout = (1, 3)\n",
    "    raw  = plt.subplot2grid(layout, (0, 0))\n",
    "    acf  = plt.subplot2grid(layout, (0, 1))\n",
    "    pacf = plt.subplot2grid(layout, (0, 2))\n",
    "    \n",
    "    raw.plot(data)\n",
    "    sm.tsa.graphics.plot_acf(data, lags=lags, ax=acf, zero=False)\n",
    "    sm.tsa.graphics.plot_pacf(data, lags=lags, ax=pacf, zero = False)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4b578826-c267-4801-9b16-f10ba8c3e725"
    }
   },
   "outputs": [],
   "source": [
    "# helper plot for monthly temps\n",
    "plots(monthly_temp.temp, lags=36);\n",
    "# open Duke guide for visual\n",
    "# we note a 12-period cycle (yearly) with suspension bridge design, so must use SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b352818a-a4b5-4348-b636-c43a99a142fd"
    }
   },
   "source": [
    "### [Box-Jenkins Method](https://en.wikipedia.org/wiki/Box–Jenkins_method)\n",
    "\n",
    "ACF Shape|Indicated Model\n",
    "---|---\n",
    "Exponential, decaying to zero|Autoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model.\n",
    "Alternating positive and negative, decaying to zero|Autoregressive model. Use the partial autocorrelation plot to help identify the order.\n",
    "One or more spikes, rest are essentially zero|Moving average model, order identified by where plot becomes zero.\n",
    "Decay, starting after a few lags|Mixed autoregressive and moving average (ARMA) model.\n",
    "All zero or close to zero|Data are essentially random.\n",
    "High values at fixed intervals|Include seasonal autoregressive term.\n",
    "No decay to zero|Series is not stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6ffff0cc-e623-48ee-923c-94cd890ea7bf"
    }
   },
   "outputs": [],
   "source": [
    "# we might need to install dev version for statespace functionality\n",
    "#!pip install git+https://github.com/statsmodels/statsmodels.git\n",
    "\n",
    "# fit SARIMA monthly based on helper plots\n",
    "sar = sm.tsa.statespace.SARIMAX(monthly_temp.temp, \n",
    "                                order=(1,0,0), \n",
    "                                seasonal_order=(0,1,1,12), \n",
    "                                trend='c').fit()\n",
    "sar.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "83ad04e8-0cd1-4687-aa59-3c109a329f6e"
    }
   },
   "outputs": [],
   "source": [
    "# plot resids\n",
    "plots(sar.resid[sar.loglikelihood_burn:], lags=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought process:**  \n",
    "\n",
    "010010 is probably overdifferenced as we can see by negative ACF at lag 1\n",
    "\n",
    "000010 is a big underdiff at seasonal lag, but with better AIC\n",
    "\n",
    "Looks like 000010,12 and Trend='c' per rule\n",
    "\n",
    "Looking back at seasonal we notice negative ACR spike at 12: we will thus add a SMA term and we see a big drop in AIC to 4289\n",
    "\n",
    "looks like ACF looks good at seasonal lags, so we move back to ARIMA portion.\n",
    "\n",
    "ACF shows we can use AR terms. AR=1,2 or 3 have similar AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2ca50344-0a60-4cf7-b943-97e7c7d42342"
    }
   },
   "outputs": [],
   "source": [
    "# plot residual diagnostics\n",
    "sar.plot_diagnostics(lags=12,figsize = (20,10),);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ad25f01b-d0e5-4db7-ad19-8d100adc0546"
    }
   },
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "#use model.predict() start and end in relation to series\n",
    "monthly_temp['forecast'] = sar.predict(start = 750, end= 790)  \n",
    "monthly_temp[730:][['temp', 'forecast']].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introducing another model\n",
    "sar2 = sm.tsa.statespace.SARIMAX(monthly_temp.temp, \n",
    "                                order=(3,0,0), \n",
    "                                seasonal_order=(0,1,1,12), \n",
    "                                trend='c').fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "monthly_temp['forecast'] = sar2.predict(start = 750, end= 790, dynamic=False)  \n",
    "plt.plot(monthly_temp[730:][['temp', 'forecast']])\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use get forecast to create a forecast object\n",
    "future_fcst = sar2.get_forecast(50)\n",
    "# That will have a method to pull in confidence interval \n",
    "confidence_int = future_fcst.conf_int(alpha = 0.01)\n",
    "# Has an attribute to pull in predicted mean\n",
    "fcst = future_fcst.predicted_mean\n",
    "# Plot predictions and confidence intervals\n",
    "plt.plot(monthly_temp.temp[-50:])\n",
    "plt.plot(fcst)\n",
    "plt.fill_between(confidence_int.index,confidence_int['lower temp'],confidence_int['upper temp'],alpha = 0.5)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c5b280b7-81d2-45ec-9fbf-85d6fe38bb46"
    }
   },
   "source": [
    "## Section 3: Statistical Tests\n",
    "\n",
    "\n",
    "- [Normality (Jarque-Bera)](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_normality.html#statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_normality)\n",
    "    - Null hypothesis is normally distributed residuals (good, plays well with RMSE and similar error metrics)\n",
    "\n",
    "- [Serial correlation (Ljung-Box)](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_serial_correlation.html#statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_serial_correlation)\n",
    "    - Null hypothesis is no serial correlation in residuals (independent of each other)\n",
    "\n",
    "- [Heteroskedasticity](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_heteroskedasticity.html#statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_heteroskedasticity)\n",
    "    - Tests for change in variance between residuals.\n",
    "    - The null hypothesis is no heteroskedasticity. That means different things depending on which alternative is selected:\n",
    "        - Increasing: Null hypothesis is that the variance is not increasing throughout the sample; that the sum-of-squares in the later subsample is not greater than the sum-of-squares in the earlier subsample.\n",
    "        - Decreasing: Null hypothesis is that the variance is not decreasing throughout the sample; that the sum-of-squares in the earlier subsample is not greater than the sum-of-squares in the later subsample.\n",
    "        - Two-sided (default): Null hypothesis is that the variance is not changing throughout the sample. Both that the sum-of-squares in the earlier subsample is not greater than the sum-of-squares in the later subsample and that the sum-of-squares in the later subsample is not greater than the sum-of-squares in the earlier subsample.\n",
    "\n",
    "- [Durbin Watson](https://en.wikipedia.org/wiki/Durbin–Watson_statistic)\n",
    "    - Tests autocorrelation of residuals: we want between 1-3, 2 is ideal (no serial correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar.test_normality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "beffe543-0e64-4f9f-8794-981d5b017e04"
    }
   },
   "outputs": [],
   "source": [
    "# create and run statistical tests on model\n",
    "norm_val, norm_p, skew, kurtosis = sar.test_normality('jarquebera')[0]\n",
    "lb_val, lb_p = sar.test_serial_correlation(method='ljungbox',)[0]\n",
    "het_val, het_p = sar.test_heteroskedasticity('breakvar')[0]\n",
    "\n",
    "\n",
    "# we want to look at largest lag for Ljung-Box, so take largest number in series\n",
    "# there's intelligence in the method to determine how many lags back to calculate this stat\n",
    "lb_val = lb_val[-1]\n",
    "lb_p = lb_p[-1]\n",
    "durbin_watson = sm.stats.stattools.durbin_watson(\n",
    "    sar.filter_results.standardized_forecasts_error[0, sar.loglikelihood_burn:])\n",
    "\n",
    "print('Normality: val={:.3f}, p={:.3f}'.format(norm_val, norm_p));\n",
    "print('Ljung-Box: val={:.3f}, p={:.3f}'.format(lb_val, lb_p));\n",
    "print('Heteroskedasticity: val={:.3f}, p={:.3f}'.format(het_val, het_p));\n",
    "print('Durbin-Watson: d={:.2f}'.format(durbin_watson))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "17940974-3ca2-4e9b-9d36-fed9df31ab0e"
    }
   },
   "source": [
    "### Note on autofit methods\n",
    "R has an autoARIMA function (and other automagic methods) that gridsearches/optimizes our model hyperparameters for us. Over time, more of these goodies are porting to Python (e.g. pmdarima). While there's nothing wrong with utilizing these resources, the _human makes the final determination!_ Don't become over-reliant on these methods, especially early on when you are grasping the underlying mechanics and theory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyramid.arima import auto_arima\n",
    "stepwise_model = pm.auto_arima(monthly_temp.temp, start_p=1, start_q=1,\n",
    "                           max_p=3, max_q=3, m=12,\n",
    "                           start_P=0, seasonal=True,\n",
    "                           d=0, D=1, trace=True,\n",
    "                           error_action='ignore',  \n",
    "                           suppress_warnings=True, \n",
    "                           stepwise=True)\n",
    "print(stepwise_model.aic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "def future_preds_df(model,series,num_months):\n",
    "    pred_first = series.index.max()+relativedelta(months=1)\n",
    "    pred_last = series.index.max()+relativedelta(months=num_months)\n",
    "    date_range_index = pd.date_range(pred_first,pred_last,freq = 'MS')\n",
    "    vals = model.predict(n_periods = num_months)\n",
    "    return pd.DataFrame(vals,index = date_range_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = future_preds_df(stepwise_model,monthly_temp.temp,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(monthly_temp.temp)\n",
    "plt.plot(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_model.plot_diagnostics();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('auto-fit order: :', stepwise_model.order)\n",
    "print('auto-fit seasonal_order: :', stepwise_model.seasonal_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When deciding on a model, often what truly matters is how well we would be able to produce out of sample predictions.\n",
    "Here we create a function that looks at multiple out of sample predictions to see which model had lowest out of sample error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(series,horizon,start,step_size,order = (1,0,0),seasonal_order = (0,0,0,0),trend=None):\n",
    "    '''\n",
    "    Function to determine in and out of sample testing of arima model    \n",
    "    \n",
    "    arguments\n",
    "    ---------\n",
    "    series (seris): time series input\n",
    "    horizon (int): how far in advance forecast is needed\n",
    "    start (int): starting location in series\n",
    "    step_size (int): how often to recalculate forecast\n",
    "    order (tuple): (p,d,q) order of the model\n",
    "    seasonal_order (tuple): (P,D,Q,s) seasonal order of model\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame: gives fcst and actuals with date of prediction\n",
    "    '''\n",
    "    fcst = []\n",
    "    actual = []\n",
    "    date = []\n",
    "    for i in range(start,len(series)-horizon,step_size):\n",
    "        model = sm.tsa.statespace.SARIMAX(series[:i+1], #only using data through to and including start \n",
    "                                order=order, \n",
    "                                seasonal_order=seasonal_order, \n",
    "                                trend=trend).fit()\n",
    "        fcst.append(model.forecast(steps = horizon)[-1]) #forecasting horizon steps into the future\n",
    "        actual.append(series[i+horizon]) # comparing that to actual value at that point\n",
    "        date.append(series.index[i+horizon]) # saving date of that value\n",
    "    return pd.DataFrame({'fcst':fcst,'actual':actual},index=date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "series = monthly_temp.temp\n",
    "horizon = 12\n",
    "start = 700\n",
    "step_size = 3\n",
    "order = (1,0,0)\n",
    "seasonal_order = (0,1,1,12)\n",
    "\n",
    "cv1 = cross_validate(monthly_temp.temp,12,700,3,\n",
    "                    order = order,\n",
    "                    seasonal_order = seasonal_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example to see underpinning of cv\n",
    "\n",
    "\n",
    "model = sm.tsa.statespace.SARIMAX(series[:701], #only using data through start of 700\n",
    "                                order=order, \n",
    "                                seasonal_order=seasonal_order, \n",
    "                                trend=None).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end of input\n",
    "series[:701].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value to predict horizon steps into the future\n",
    "series[712:713]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what model predicted for that date\n",
    "model.forecast(12)[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1.plot(title = 'forecast every three months using one year prior data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining an error metric to see out of sample accuracy\n",
    "def mape(df_cv):\n",
    "    return abs(df_cv.actual - df_cv.fcst).sum() / df_cv.actual.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "series = monthly_temp.temp\n",
    "horizon = 12\n",
    "start = 700\n",
    "step_size = 3\n",
    "order = (1,1,0)\n",
    "seasonal_order = (0,1,1,12)\n",
    "\n",
    "cv2 = cross_validate(monthly_temp.temp,12,700,3,\n",
    "                    order = order,\n",
    "                    seasonal_order = seasonal_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.plot(title = 'forecast every three months using one year prior data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(monthly_temp.temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_ARIMA(series,horizon,start,step_size,orders = [(1,0,0)],seasonal_orders = [(0,0,0,0)],trends=[None]):\n",
    "    best_mape = np.inf\n",
    "    best_order = None\n",
    "    best_seasonal_order = None\n",
    "    best_trend = None\n",
    "    for order_ in orders:\n",
    "        for seasonal_order_ in seasonal_orders:\n",
    "            for trend_ in trends:\n",
    "                \n",
    "                cv = cross_validate(series,\n",
    "                                    horizon,\n",
    "                                    start,\n",
    "                                    step_size,\n",
    "                                    order = order_,\n",
    "                                    seasonal_order = seasonal_order_,\n",
    "                                    trend=trend_)\n",
    "                if mape(cv)<best_mape:\n",
    "                    best_mape = mape(cv)\n",
    "                    best_order = order_\n",
    "                    best_seasonal_order = seasonal_order_\n",
    "                    best_trend = trend_\n",
    "    return (best_order,best_seasonal_order, best_trend, best_mape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = monthly_temp.temp\n",
    "horizon = 12\n",
    "start = 760\n",
    "step_size = 3\n",
    "orders = [(1,1,0),(1,0,0)]\n",
    "seasonal_orders = [(0,1,1,12)]\n",
    "trends = [None,'c']\n",
    "\n",
    "grid_search_ARIMA(series = series,\n",
    "                  horizon = horizon,\n",
    "                  start = start,\n",
    "                  step_size = step_size,\n",
    "                  orders = orders,\n",
    "                  seasonal_orders = seasonal_orders,\n",
    "                  trends=trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bf0fbe23-2778-4730-ac8b-5ebd0adb1334"
    }
   },
   "source": [
    "### Forecasting Exercise:\n",
    "1. Find appropriate model to forecast sunspots data\n",
    "2. Run diagnostics to check if fit makes sense\n",
    "3. Run cross-validation on a few different choices to see which one has best out of sample error\n",
    "\n",
    "**Note:** This data is annual, but we still need to find the seasonality\n",
    "The variance does not seem to stay constant throughout, think of transformations we discussed earlier in course to take care of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "548737a2-0250-46b1-87db-7ad700847678"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# read and plot data\n",
    "data_path = 'https://vincentarelbundock.github.io/Rdatasets/csv/datasets/sunspot.year.csv'\n",
    "data = pd.read_csv(data_path,usecols = ['time','value'],index_col = 'time',parse_dates=['time'])\n",
    "plt.figure()\n",
    "plt.plot(data.index,data['value'])\n",
    "plt.ylabel('Sunspots')\n",
    "plt.title('Yearly Sunspot Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given the difference in variance we should probably take log of data\n",
    "data['log_ss'] = np.log1p(data['value'])\n",
    "# going to zoom in on last 60 values to get a better idea of frequency of seasonality\n",
    "plt.plot(data['log_ss'][:60])\n",
    "plt.xticks(ticks = data.iloc[0:60:11].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decomposition with frequency 11\n",
    "# Seems to do decent job of capturing seasonality\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "seasonal_decompose(data.log_ss,freq=11).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.tsa.graphics.plot_acf(data.log_ss,zero=False)\n",
    "sm.tsa.graphics.plot_pacf(data.log_ss,zero = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lag_11'] = data.log_ss.shift(11)\n",
    "data['seasonal_diff'] = data.log_ss - data['lag_11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean moves, not perfect. p-value shows we can reject the null of non stationarity\n",
    "dftest(data['seasonal_diff'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.tsa.graphics.plot_acf(data['seasonal_diff'].dropna(),zero=False)\n",
    "sm.tsa.graphics.plot_pacf(data['seasonal_diff'].dropna(),zero=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like an ar2 model with seasonal differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar3 = sm.tsa.statespace.SARIMAX(data.log_ss, \n",
    "                                order=(0,0,0), \n",
    "                                seasonal_order=(0,1,0,12), \n",
    "                                trend='c').fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.tsa.graphics.plot_pacf(sar3.resid[sar3.loglikelihood_burn:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar3.plot_diagnostics(figsize = (15,8),lags = range(1,15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_model = pm.auto_arima(data.log_ss, start_p=0, start_q=0,\n",
    "                           max_p=3, max_q=3, m=11,\n",
    "                           start_P=0, seasonal=True,\n",
    "                           d=0, D=1, trace=True,\n",
    "                           error_action='ignore',  \n",
    "                           suppress_warnings=True, \n",
    "                           stepwise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('order: ',auto_model.order)\n",
    "print('seasonal order: ',auto_model.seasonal_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar4 = sm.tsa.statespace.SARIMAX(data.log_ss, \n",
    "                                order=(2,0,0), \n",
    "                                seasonal_order=(0,1,0,12), \n",
    "                                trend='c').fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar4.plot_diagnostics(lags=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "series = data['log_ss']\n",
    "horizon = 3\n",
    "start = int(len(data.value)*.75)\n",
    "step_size = 1\n",
    "order = auto_model.order\n",
    "seasonal_order = auto_model.seasonal_order\n",
    "\n",
    "log_cv1 = cross_validate(series,horizon,start,step_size,\n",
    "                    order = order,\n",
    "                    seasonal_order = seasonal_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cv1 = np.expm1(log_cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log_cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(log_cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_model.plot_diagnostics(figsize = (15,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "series = data['log_ss']\n",
    "horizon = 3\n",
    "start = int(len(data.value)*.75)\n",
    "step_size = 1\n",
    "order = (2,0,0)\n",
    "seasonal_order = (1,1,0,11)\n",
    "\n",
    "log_cv2 = cross_validate(series,horizon,start,step_size,\n",
    "                    order = order,\n",
    "                    seasonal_order = seasonal_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cv2 = np.expm1(log_cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_cv2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(log_cv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "51115ac6-e0f0-4502-a4fa-de52a936e5fe"
    }
   },
   "source": [
    "# Section 6: Predicting with [Facebook](https://facebookincubator.github.io/prophet/) [Prophet](https://research.fb.com/prophet-forecasting-at-scale/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ca02a194-adb8-4f0e-ba49-a20a29d1798d"
    }
   },
   "source": [
    "From site:\n",
    "\n",
    "> Today Facebook is open sourcing Prophet, a forecasting tool available in Python and R. The idea is that producing high quality forecasts is not an easy problem for either machines or for most analysts. The models revolves around two main observations in the practice of creating a variety of business forecasts:\n",
    "- Completely automatic forecasting techniques can be brittle and they are often too inflexible to incorporate useful assumptions or heuristics.\n",
    "- Analysts who can produce high quality forecasts are quite rare because forecasting is a specialized data science skill requiring substantial experience.\n",
    "\n",
    "Prophet is an general additive model that includes a number of highly advanced, intelligent [forecasting methods](http://andrewgelman.com/2017/03/01/facebooks-prophet-uses-stan/), including [changepoint analysis](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0ahUKEwjpwM_JhsbVAhWNxIMKHSWqD6kQFgguMAE&url=http%3A%2F%2Fwww.variation.com%2Fcpa%2Ftech%2Fchangepoint.html&usg=AFQjCNFK6wbwWuBCixZJHu03LkABXL3UHA):\n",
    "\n",
    "\n",
    "_y = g(t) + s(t) + h(t) + $\\epsilon_t$_\n",
    "\n",
    "Here g(t) is the trend function which models non-periodic changes in the value of the time series, s(t) represents periodic changes (e.g., weekly and yearly seasonality), and h(t) represents the effects of holidays which occur on potentially irregular schedules over\n",
    "one or more days\n",
    "\n",
    "- For trend, a piecewise linear or logistic growth curve trend is used. \n",
    "    - Prophet automatically detects changes in trends by selecting changepoints from the data.\n",
    "- For seasonalities, different seasonality components are modeled using Fourier series.\n",
    "- One can either use fb provided list or incorporate their own holidays into model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "add4561b-f272-4352-b493-f7f854ad1513"
    }
   },
   "source": [
    "Prophet was originally optimized with the business forecast tasks encountered at Facebook in mind, which typically have any of the following characteristics:\n",
    "\n",
    "- Hourly, daily, or weekly observations with at least a few months (preferably a year) of history\n",
    "- Strong multiple “human-scale” seasonalities: day of week and time of year\n",
    "- Important holidays that occur at irregular intervals that are known in advance\n",
    "- A reasonable number of missing observations or large outliers\n",
    "- Historical trend changes, for instance due to product launches or logging changes\n",
    "- Trends that are non-linear growth curves, where a trend hits a natural limit or saturates\n",
    "\n",
    "\n",
    "[Technical details behind prophet](https://facebookincubator.github.io/prophet/static/prophet_paper_20170113.pdf): built around a generalized additive model (GAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "nbpresent": {
     "id": "8a028a7a-a6e9-4804-915d-fd6c37a6962d"
    }
   },
   "outputs": [],
   "source": [
    "# read daily page views for the Wikipedia page for Peyton Manning; scraped into hosted CSV\n",
    "# conda install -c conda-forge fbprophet (to install)\n",
    "from fbprophet import Prophet\n",
    "plt.rcParams['figure.figsize'] = [14, 4]\n",
    "\n",
    "data_path = 'https://raw.githubusercontent.com/PinkWink/DataScience/master/data/07.%20example_wp_peyton_manning.csv'\n",
    "peyton = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f9f19738-bbd3-4619-8e52-ea341c3be9e7"
    }
   },
   "outputs": [],
   "source": [
    "# plot data\n",
    "peyton.plot()\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ee67d09b-da9f-446f-b625-cd6f4b4a73c2"
    }
   },
   "outputs": [],
   "source": [
    "# log data due to spikes\n",
    "# dataframe must have ds column with type datetime and y column which is time series we are trying to predict\n",
    "peyton['y'] = np.log(peyton['y'])\n",
    "peyton.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "47c8f1d9-0612-4fa5-a1f4-c9acccf8d1c6"
    }
   },
   "outputs": [],
   "source": [
    "# plot log\n",
    "peyton.plot()\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "baeb2d37-41f7-4b40-939f-a479bc8326d3"
    }
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "m = Prophet()\n",
    "m.fit(peyton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peyton.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "af976eea-a7c3-4fae-9ac8-f7abaabf29ac"
    }
   },
   "outputs": [],
   "source": [
    "# forecast 365 days into future\n",
    "# prophet requires a blank dataframe to input predictions\n",
    "# will also provide blank set for dates within dataset to allow for fit\n",
    "future = m.make_future_dataframe(periods=365)\n",
    "print(future.head())\n",
    "print(future.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "66558e86-2e7a-4585-ab24-c5c5009e2676"
    }
   },
   "outputs": [],
   "source": [
    "# populate forecast\n",
    "forecast = m.predict(future)\n",
    "print(forecast.columns)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "90230a24-4d1d-4bb5-af90-364fdc5dc46b"
    }
   },
   "outputs": [],
   "source": [
    "# plot forecast\n",
    "m.plot(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e4c6c458-5bde-43ad-848a-16478569f6f8"
    }
   },
   "outputs": [],
   "source": [
    "# plot individual components of forecast: trend, weekly/yearly seasonality,\n",
    "m.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4e554cc3-a53f-4a94-9dd0-0e69e70beef9"
    }
   },
   "source": [
    ">We can also add holiday and Superbowl date information to Peyton's forecast, since we hypothesize people will visit his site more often on those dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "nbpresent": {
     "id": "b43cd850-f8fa-456a-a445-39686e5c5b8e"
    }
   },
   "outputs": [],
   "source": [
    "# add holidays \n",
    "playoffs = pd.DataFrame({\n",
    "  'holiday': 'playoff',\n",
    "  'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16',\n",
    "                        '2010-01-24', '2010-02-07', '2011-01-08',\n",
    "                        '2013-01-12', '2014-01-12', '2014-01-19',\n",
    "                        '2014-02-02', '2015-01-11', '2016-01-17',\n",
    "                        '2016-01-24', '2016-02-07']),\n",
    "  'lower_window': 0, # these help us specify spillover into previous and future days which will be treated as own holidays\n",
    "  'upper_window': 1,\n",
    "})\n",
    "\n",
    "superbowls = pd.DataFrame({\n",
    "  'holiday': 'superbowl',\n",
    "  'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']),\n",
    "  'lower_window': 0,\n",
    "  'upper_window': 1,\n",
    "})\n",
    "\n",
    "holidays = pd.concat((playoffs, superbowls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b0f87b06-75cf-433f-9820-109cb5939753"
    }
   },
   "outputs": [],
   "source": [
    "# fit and predict\n",
    "m = Prophet(holidays=holidays)\n",
    "forecast = m.fit(peyton).predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c565849d-43a8-4453-a4f5-8ce8065756a9"
    }
   },
   "outputs": [],
   "source": [
    "# we can see the effects of various 'holidays' on site visits\n",
    "forecast[(forecast['playoff'] + forecast['superbowl']).abs() > 0][\n",
    "        ['ds', 'playoff', 'superbowl']][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f728ec7e-bbe1-4963-9f2c-8b9ca84a46de"
    }
   },
   "outputs": [],
   "source": [
    "# check the impacts visually\n",
    "m.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a283c343-2c68-48a5-9276-16a9345be058"
    }
   },
   "source": [
    ">Peyton won Superbowls XLI (41, 2007) and 50 (2016), while losing XLIV (44, 2010) and XLVIII(48, 2014). We can see these spikes in the holidays chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in another seasonality besides yearly, weekly, daily\n",
    "# fit model\n",
    "m = Prophet(holidays=holidays,)\n",
    "m.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
    "m.fit(peyton)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst_month = m.predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_components(fcst_month)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating indicator variable for nfl sundays\n",
    "def nfl_sunday(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    if date.weekday() == 6 and (date.month > 8 or date.month < 2):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#adding that to our df\n",
    "peyton['nfl_sunday'] = peyton['ds'].apply(nfl_sunday)\n",
    "print(peyton)\n",
    "\n",
    "m = Prophet()\n",
    "\n",
    "# must be in the fit df\n",
    "m.add_regressor('nfl_sunday')\n",
    "m.fit(peyton)\n",
    "\n",
    "# regressor must also be available in future df\n",
    "future['nfl_sunday'] = future['ds'].apply(nfl_sunday)\n",
    "\n",
    "forecast = m.predict(future)\n",
    "fig = m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are points where trend has changed\n",
    "print('originally: ',m.changepoints[:5])\n",
    "# you can specify changepoints if you want trend to only be allowed at certain points\n",
    "m_c = Prophet(changepoints=['2014-01-01'])\n",
    "\n",
    "print('\\nnow: ',m_c.changepoints[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation with fbprophet\n",
    "<img src=\"data/diagnostics_3_0.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import cross_validation\n",
    "#Starting from 730 days in, making a prediction every 180 days, 365 days into the future\n",
    "df_cv = cross_validation(m, initial='730 days', period='180 days', horizon = '365 days')\n",
    "df_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just looking at data from first cutoff\n",
    "first_cut = df_cv[df_cv.cutoff == datetime(2010,2,15)]\n",
    "plt.plot(first_cut.ds,first_cut.y,label='actual')\n",
    "plt.plot(first_cut.ds,first_cut.yhat,label = 'pred')\n",
    "plt.fill_between(first_cut.ds,first_cut.yhat_lower,first_cut.yhat_upper,alpha=0.4)\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import performance_metrics\n",
    "df_p = performance_metrics(df_cv)\n",
    "df_p.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e3e01320-3a45-487f-a826-86757be7cae4"
    }
   },
   "source": [
    "### Predicting CO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "co2 = pd.read_csv('./co2-ppm-mauna-loa-19651980.csv', \n",
    "                  header = 0,\n",
    "                  names = ['idx', 'co2'],\n",
    "                  skipfooter = 2)\n",
    "co2 = co2.drop('idx', 1)\n",
    "\n",
    "# recast co2 col to float\n",
    "co2['co2'] = pd.to_numeric(co2['co2'])\n",
    "co2.drop(labels=0, inplace=True)\n",
    "\n",
    "# set index\n",
    "index = pd.date_range('1/1/1965', periods=191, freq='M')\n",
    "co2.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4ef59c0f-6949-43bb-aa69-88f4d1755214"
    }
   },
   "outputs": [],
   "source": [
    "# load co2 data, rename headers, and check\n",
    "# data = sm.datasets.co2.load_pandas()\n",
    "# co2 = data.data\n",
    "\n",
    "co2['ds'] = co2.index\n",
    "co2.rename(columns={'co2': 'y'}, inplace=True)\n",
    "\n",
    "co2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "cc7e92ae-4752-432a-b2be-2e676a4113e2"
    }
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "model = Prophet()\n",
    "model.fit(co2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b2097b62-1a95-4286-86bb-1c60dcc3913d"
    }
   },
   "outputs": [],
   "source": [
    "# forecast 15 years into future\n",
    "future = model.make_future_dataframe(periods=120, freq='M', include_history=True)\n",
    "future.tail()\n",
    "\n",
    "#future = model.make_future_dataframe(periods=365*15)\n",
    "#future.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e24731c2-c36c-4d81-86d1-8572d93b9c1d"
    }
   },
   "outputs": [],
   "source": [
    "# populate forecast\n",
    "forecast = model.predict(future)\n",
    "forecast.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "08e618cb-91fb-4652-aca8-c7cef4b999d6"
    }
   },
   "outputs": [],
   "source": [
    "model.plot(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "cbf53b27-bc78-4697-9a02-5de5a5320980"
    }
   },
   "outputs": [],
   "source": [
    "# plot individual components of forecast: trend, weekly/yearly seasonality,\n",
    "model.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to look into adjusting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decreasted drastically from defaults\n",
    "changepoint_prior_scale = 0.05\n",
    "seasonality_prior_scale = 0.00001\n",
    "\n",
    "growth='logistic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "edf3c59f-0d6e-41bd-ad17-c390d6a5060e"
    }
   },
   "outputs": [],
   "source": [
    "# we can add a cap to limit our theoretical growth if we are using logistic growth\n",
    "co2['cap'] = 350\n",
    "m = Prophet(growth=growth, #weekly_seasonality=10000,\n",
    "            \n",
    "            seasonality_prior_scale=seasonality_prior_scale,\n",
    "            changepoint_prior_scale=changepoint_prior_scale)\n",
    "m.fit(co2);\n",
    "\n",
    "# forecast 15 years into future with cap of 380\n",
    "future = m.make_future_dataframe(periods=120,freq='M', include_history=False)\n",
    "future['cap'] = 350\n",
    "\n",
    "forecast = m.predict(future)\n",
    "m.plot(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "* Create prediction using hourly data of PM_Dongsi for last day half of december of 2015\n",
    "    * Note you will have to use what we first learned to convert the year, month, day, hour columns to datetime object\n",
    "* Plot daily and weekly seasonality of forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Beijing = pd.read_csv('./FiveCitiesPM/Beijing.csv')\n",
    "df_Beijing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructor solution\n",
    "# Create a new column that is a datetime object\n",
    "def make_date(row):\n",
    "    return datetime(year = row['year'], month = row['month'], day = row['day'], hour = row['hour'])\n",
    "\n",
    "df_Beijing['date'] = df_Beijing.apply(make_date,axis=1)\n",
    "# Make index for easy indexing of time values\n",
    "\n",
    "df_Beijing.set_index('date',inplace=True)\n",
    "df_Beijing['ds'] = df_Beijing.index\n",
    "\n",
    "# Only take required fields\n",
    "df = df_Beijing[[\"ds\",'PM_Dongsi']]\n",
    "df.rename(columns = {'PM_Dongsi':'y'},inplace=True)\n",
    "\n",
    "# create a training set and a test set. We are only going to use last month's data to make everything a bit more clear\n",
    "df_train = df['2015-11']\n",
    "df_test = df['2015-12':'2015-12-15']\n",
    "print(df_train.tail())\n",
    "print(df_test.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "m = Prophet()\n",
    "m.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods = 15*24,freq = 'h') # could also leave default freq of days and do 31 for period\n",
    "print(future.head())\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate forecast\n",
    "forecast = m.predict(future)\n",
    "print(forecast.columns)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot(forecast)\n",
    "plt.plot(df_test.y,'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decreasted drastically from defaults\n",
    "changepoint_prior_scale = 0.0005\n",
    "seasonality_prior_scale = 10\n",
    "\n",
    "\n",
    "m = Prophet(daily_seasonality=10,\n",
    "            \n",
    "            seasonality_prior_scale=seasonality_prior_scale,\n",
    "            changepoint_prior_scale=changepoint_prior_scale)\n",
    "m.fit(df_train);\n",
    "\n",
    "\n",
    "forecast = m.predict(future)\n",
    "m.plot(forecast)\n",
    "plt.plot(df_test.y,'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0cde930b-ac3c-4032-9dc0-261c37a52306"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2f8f0b11-e363-4227-8803-df1cba7c5f93"
    }
   },
   "source": [
    "# Summary\n",
    "In this notebook, we have covered: \n",
    "1. A practical understanding of Autoregressive Moving Average (ARMA) models.\n",
    "2. A basic understanding of the Autocorrelation Function (ACF).\n",
    "3. Insight into choosing the order *q* of MA models.\n",
    "4. A practical understanding of Autoregressive (AR) models.\n",
    "5. A basic understanding of the Partial Autocorrelation Function (PACF).\n",
    "6. Insight into choosing the order *p* of AR models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": false,
   "nav_menu": {
    "height": "311px",
    "width": "412px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": "3",
   "toc_cell": false,
   "toc_position": {
    "height": "22px",
    "left": "1105px",
    "right": "20px",
    "top": "-1px",
    "width": "22px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
